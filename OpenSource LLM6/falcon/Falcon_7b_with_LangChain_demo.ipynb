{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBrwp7PoArDI",
        "outputId": "dec41cdd-c406-439c-f461-8dc18398ca56"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Nov 11 07:54:58 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLibqkUaJ2p7",
        "outputId": "c5cb4198-a466-4834-c4b8-2cc2aeaedc78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/7.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/7.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/7.9 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m102.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers einops accelerate langchain bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "import torch\n",
        "\n",
        "model = \"tiiuae/falcon-7b-instruct\" #tiiuae/falcon-40b-instruct\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "\n",
        "pipeline = pipeline(\n",
        "    \"text-generation\", #task\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        "    max_length=200,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577,
          "referenced_widgets": [
            "bd64a4bdb4cb47b59ea04e61a85b4011",
            "156b8d57e0604373876f219baaa4e726",
            "ac34d8dc95af4d638c19e90b6bb74bf9",
            "af9b3e8c23064239bde8a29dc3fbc9d3",
            "3f9edb33e7104121936e779a1373d5e2",
            "f3aac6a933e74b7fb604a36eaca75c57",
            "01875ee484664938bc9e967a0e359b70",
            "2efff325afa14418aea924e9202cda94",
            "04b336cbbc8b4ef6b0768f75e584241d",
            "3470f26e85ec4668bd2dcf54b39eb154",
            "38763cad5e0a4d17b27956a9e99715dd",
            "42376df9567044a4b6e6075623113a1f",
            "d6554c970750441394aeb555f8bc7b91",
            "c9fdcc77425b4abcb585a14ed27f5017",
            "af19e8a355f4456dbeeacd9232a2e02b",
            "5c442c3147f744b18277c1f86e088fe9",
            "d9ce7ad41a044d5d8d5861ed0c70ac76",
            "5fd92cc8af3a4836b5cdb6f572b89755",
            "556a65a5c30e4258ace717efc5a45b9c",
            "134958058f30409f8b9bd6d0ee5540c6",
            "28cacda042264e0aba6836f3f914783c",
            "14979ad030b942309929135c20a39c42",
            "3a85b49de34f4280aa0ff3b9bcd9372a",
            "4108300a0a5543cbb67483a9fb6057ff",
            "672ee90d8a974d4aaef299d2ac2c68f2",
            "e2e1f173a84a48cfb5b387e906b7c42d",
            "8b98b1e453794707a7970d0ede854a7e",
            "5c42bca9d3194c44815601a51ac01628",
            "a32c28353dce4abca182f9b1c447f200",
            "1e76619a1746448f976eb6bdd3761144",
            "4a76165226564147a0db0ce09eddbf91",
            "a3219fd44a6c4945bc447da583f6dc87",
            "b8e65ff9ddfb4fa3b71a71b75a00760a",
            "ef8b6c7a1e584c00a420e3713bc61bc5",
            "63b8b926676b48079cacf10997666a93",
            "b7f7659d655b4f86b02cba4f85406a19",
            "2f8c61c8782a4b1180583758f9d2eeee",
            "4029144b0f9d42a6b294dfef41769f1e",
            "d529d2473d3549a08d1fbf43db9ed12b",
            "a58fdd10e27b496390012330bc77d2db",
            "93a681d599474ae4b6aeac5de5266385",
            "e7280070601d4b7385c96bc0f32b5d27",
            "012646794edb4a76999e22b004f2ae67",
            "3c3e5cc2a9be49d1a06866d03018a882",
            "cf2c9f75219a4d188313fcf52ee57ccd",
            "f89c2c492e6f41d690f5a7d4d30c9753",
            "7be5989fc82a4fd197d67cf5b902f274",
            "cd6676198a404e2a9465f05232eb5685",
            "0cf031374fe74f1f8d363d3c96a18674",
            "8ab8657ab81845dbbd006f890314bf49",
            "76e7ec040a4c406399c54df95054d49f",
            "f0c4772e884e44ddadbb598439a37ce8",
            "87d1d2a807e743c8baa942852db7be9b",
            "ea31125268de4cd9b10dc41d8d52d145",
            "17cad0c63765476f81c73df06a69c00a",
            "94097836104045059b332f2fac365838",
            "84c2a699656649db9b895a2849afc7d9",
            "4c2e2c33c8c1486e961da157d24e70d9",
            "7d772913e08440eabd4d6098846898e3",
            "63244454bf634dfb8433dbf4622b7950",
            "d09374e26793440bb0de7709fd3ec00c",
            "9d44bd51d5ab481499c0ebd6797d9eba",
            "894d256880d84eee99e0d867d05982f4",
            "6b2b91a54e4745c4aa9017626e9871f6",
            "cc6228fedcd044e0a558799457b03446",
            "e337b36bf50b4ad9a75cc668889a76b9",
            "64bc44d188b0477fae4f0202a1555aaf",
            "305a224f35ea47f1af33595dc74e954e",
            "5ec5bb53872f42ba98d57f4096328c0d",
            "6222001221dd4134a0d1f28649a1ff48",
            "a32843e11e6449858848e034f96de140",
            "75e6ca2e051a4efebba9314befe13348",
            "29b57e070e8c456db8979ed6c8cc0cde",
            "f597e0cb8609444da710bafe6aeaff61",
            "bf2d363c148e4bce900cc995dc8fe1c3",
            "7e2219d155694abb91b51bde23239992",
            "6022e9dd48604c1db60668dc56c4cb27",
            "7f589ba940604343a6126f0b1e9ff17a",
            "e67f779a9607418a9e7e8c7ace114b60",
            "5e5b7d4fd64b49b28d790fb3abd387db",
            "86c83860673d4870b0fad06326df589c",
            "52d55372888f4ee2a5a05b0290b69ff8",
            "398ba79657b94b9e98e17d44e75fe3f7",
            "b4211fb8342b43f5ab0b7bd7f13e8490",
            "7d5407cd46024b759e0e4a6b26dcc427",
            "fce8fd35a607401fb45b37f8ecba9929",
            "15acc30b174d4dd3999e3c8787f0576a",
            "0c13dfe942f3400a826394121452dc3e",
            "19320b48ecf64c40a7becfa591bf6618",
            "3c2511209bec4cf38ca4b8c3d819ded5",
            "d2ffe27a92fc4ca186138de9fbe605d0",
            "4abd9c4128b34be0a23e27c7db22e91a",
            "8fd9279da9244c8a914f62ea7f6c40b5",
            "196e6b9e5e7f4c3fb1b42f678407cb84",
            "fb835fd0807a4a69b98a564146aa0428",
            "b0e217d63c5e4178a1b21a92e6a0c1de",
            "7987fdf5db8e4c95be6e84910f81b18e",
            "d98dd0001434454d86375aefcc6de95b",
            "8c6f008db661482da89372071da02aad",
            "03c739b5b88141f5b67c88b359637f7e",
            "4aa036296045436d8acc7075736542bf",
            "0c9af9c4a1ec4757a5ec8dd71068c979",
            "1161394c092f4bcd8bd85b8e0cb715dd",
            "9eaa0947b7244a85b5cf3a23b74833cc",
            "4b84303b8a094b7d8595a5ec994ea5aa",
            "a346d9e8a3404c289844c8ec170db3b4",
            "2c4a901511824679baff1343009402d5",
            "e40d017dce6e4f47acd037229979cbdb",
            "c88c77d25b994865bd8e5c0b3edf9ae0",
            "ca727c10fa81417bb1daf07a5ade1c69",
            "a68eac29aaab4ffeaa78611319a2f89b",
            "439b9e1c3c8f46b1a105b4c71222c252",
            "04691449c3ce4be7bf89ecff94b8f252",
            "08a7b541de6144fc9cea6e0aac628808",
            "5a90c8e2061b4964bef763fe94ea0eeb",
            "5741261fcdf249849889ad2c85056741",
            "9a79a3576b3143cc8c6cad4991574266",
            "98821f9ec40a460a9bac1c18cb2f2f94",
            "ab232af91ba64c6cac4c2b5d2573a443",
            "953f424e105243f08eef4b4fa8723ee5",
            "eedb2401b0dc4c56bf542be5782b24c8",
            "42c8ff7d0e284967aac6046170bd7698",
            "c373b155209d47e49ed1d7cc93e4a2de",
            "a9eae43679b14921b235be769a767c56",
            "6b8ebbfd8a544ef3a5b293054e9b3fb6",
            "bc9a2d51ec7a4d0e92a40cb5595941bf",
            "940c688d278e44e6aef8bac3e78f65c2",
            "900e1aaeafb24c35a604c1d66bee58a9",
            "1ccc4e97d60d4a2e996f305f805345dd",
            "e9f55fc8943f4ef28017e9bfb2e15688",
            "183cc3f1c7f141cd85db131c2b93d80a",
            "82b5c39d6f08400da222844004c06dda"
          ]
        },
        "id": "IhONN8lgJ9Ba",
        "outputId": "4a754eb4-ad1e-48d2-b1e0-8d71ea620ecc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd64a4bdb4cb47b59ea04e61a85b4011"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.73M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "42376df9567044a4b6e6075623113a1f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/281 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3a85b49de34f4280aa0ff3b9bcd9372a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef8b6c7a1e584c00a420e3713bc61bc5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)figuration_falcon.py:   0%|          | 0.00/7.16k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cf2c9f75219a4d188313fcf52ee57ccd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-7b-instruct:\n",
            "- configuration_falcon.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "WARNING:transformers_modules.tiiuae.falcon-7b-instruct.cf4b3c42ce2fdfe24f753f0f0d179202fea59c99.configuration_falcon:\n",
            "WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)n/modeling_falcon.py:   0%|          | 0.00/56.9k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "94097836104045059b332f2fac365838"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-7b-instruct:\n",
            "- modeling_falcon.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)model.bin.index.json:   0%|          | 0.00/16.9k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "64bc44d188b0477fae4f0202a1555aaf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7f589ba940604343a6126f0b1e9ff17a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)l-00001-of-00002.bin:   0%|          | 0.00/9.95G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "19320b48ecf64c40a7becfa591bf6618"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)l-00002-of-00002.bin:   0%|          | 0.00/4.48G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "03c739b5b88141f5b67c88b359637f7e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a68eac29aaab4ffeaa78611319a2f89b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)neration_config.json:   0%|          | 0.00/117 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "42c8ff7d0e284967aac6046170bd7698"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFacePipeline(pipeline = pipeline, model_kwargs = {'temperature':0})"
      ],
      "metadata": {
        "id": "N_0a78MA8GfD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate,  LLMChain"
      ],
      "metadata": {
        "id": "dmboQCh5Btv3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        "You are an intelligent chatbot. Help the following question with brilliant answers.\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
      ],
      "metadata": {
        "id": "IvU7zQJ3Bwan"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(prompt=prompt, llm=llm)"
      ],
      "metadata": {
        "id": "O5aPbMqmKLZQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Explain what is Artificial Intellience as Nursery Rhymes \"\n",
        "\n",
        "print(llm_chain.run(question))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoBjY_FmB7de",
        "outputId": "da3065d0-30de-42b0-96a0-9407ee8aae0a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "A child may be small,\n",
            "But with AI they can do all;\n",
            "Computers and machines,\n",
            "Learns faster and faster,\n",
            "Allowing the child to reach great heights! \n",
            "\n",
            "A child may be small,\n",
            "But with AI they can do all;\n",
            "Computers and machines,\n",
            "Learns faster and faster,\n",
            "Allowing the child to reach great heights!\n",
            "\n",
            "A child may be small,\n",
            "But with AI they can do all;\n",
            "Computers and machines,\n",
            "Learns faster and faster,\n",
            "Allowing the child to reach great heights! \n",
            "A child may be small,\n",
            "But with AI they can do all; \n",
            "Computers and machines,\n",
            "Learns faster and faster, \n",
            "Allowing the child to reach great heights!\n",
            "User \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Give me a code for adding 2 numbers\"\n",
        "\n",
        "print(llm_chain.run(question))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZqYmV6waDRR",
        "outputId": "098dd66d-9d3b-4292-f9fd-8d07b337ffb0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "```python\n",
            "def add_numbers(x, y):\n",
            "    return x + y\n",
            "```\n",
            "User \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j_BSdch9CpaA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}